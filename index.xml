<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on yaotouge's place</title><link>http://yaotouge.github.io/</link><description>Recent content in Home on yaotouge's place</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 11 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="http://yaotouge.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>LeetCode die simulator</title><link>http://yaotouge.github.io/posts/die_simulator/</link><pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate><guid>http://yaotouge.github.io/posts/die_simulator/</guid><description>刷题不只是刷题，更重要的是刷各种算法题的思路（套路），找到一个宝藏up主讲的很清楚明了bilibili，茫茫刷题路呀，慢慢来吧。
今天刷每日一题，又碰到一道困难的动态规划题，一下子就懵了，跟着这位up主一起学习。一般的动态规划的步骤是：回溯 =&amp;gt; 记忆化 =&amp;gt; 递推。
回溯就是用深度优先搜索的形式把题解表示出来。 但一般直接dfs肯定会超时，所以会用数组把计算过的结果cache住，递归过程中直接使用。极大优化计算复杂度。 最后dfs是自顶向下的先递后归的过程，我们可以通过自底向上去掉“递”，只留下归的过程，也就是最终的递推关系了。 总结的步骤相当直接明了是不是，但是实际的题目千变万化，唯有勤学苦练才是。今天这道题是模拟掷骰子，老实说，如果不是看了大佬的步骤套路，如果直接一步到位出动态规划的转移方程，我是绝不可能想出来的，甚至最后得到转换的结果后，我依然无法从直观上去理解状态为什么这么转换。只能理解基本的回溯+记忆。
第一步先用回溯表达解题过程，很关键的一点是，虽然题目要求的是最后可能的骰子序列的数量，但我们依然要在模拟掷骰子的基础上来计算总数。所以掷骰子的过程中有两个关键点：
剩余的掷骰子次数，也就是n。 掷出某个点数后是否合法，这个判断依赖两个条件：上一次的点数是多少，以及上个点数的剩余连续次数是多少：如果和上次点数相同，剩余连续次数减一。否则，当前剩余次数重置为 maxRoll - 1。 当n变为0时完成一次序列生成，return 1。然后每次遍历当前可能的点数（1~6）求和得到当前的结果。由此可以写出一个比较清晰的回溯解法：
void dfs(int n, int last, int left, vector&amp;lt;int&amp;gt; &amp;amp;rollMax) { if (n == 0) return 1; int result = 0; for (int i = 0; i &amp;lt; 6; ++i) { if (i != last) result += dfs(n-1, i, rollMax[i]-1); else if (left != 0) result += dfs(n-1, i, left-1); } return result; } int dieRollSimulation(int n, vector&amp;lt;int&amp;gt;&amp;amp; rollMax) { int result = 0; for (int i = 0; i &amp;lt; 6; ++i) { result += dfs(n-1, i, rollMax[i]-1); } return result; } 但是这种写法肯定会超时，所以进行第二步记忆化，将计算过的结果缓存起来，python有个神奇的修饰器 @cache，可以根据函数的输入参数把运算结果缓存，自动完成这步：</description></item><item><title>LeetCode regular expression</title><link>http://yaotouge.github.io/posts/regular-expression-leetcode/</link><pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate><guid>http://yaotouge.github.io/posts/regular-expression-leetcode/</guid><description>最近打算刷刷题，记录一下做题的思路。现在工作形式不同了，搞技术支持代码写的少，看得多，和人打交道多。。。其实感觉自己不太合适干这个，先凑合着，先刷刷题以后再去别的地儿呢是不是。
动态规划一直没搞明白过，先从这个入手。这是原题链接，估计很多人都刷过了，我自己都做过几遍了。。。但老实说，之前看题解也是一知半解，过不了多久就忘。这次打算理解彻底一点吧。
动态规划的中心是状态转移方程，当前状态根据之前的状态得到，然后用一张表记录之前的结果。这题的思路：
f[i][j] 表示子串s[i]与子pattern p[j]的匹配结果，所以分如下情况：
p[j] == &amp;lsquo;*&amp;rsquo;，此时是0或者多次重复上一个字符，且输入保证*前面必然有字符。这是又分两种情况：
p[j-1] != s[i]，意味着上个pattern字符和s[i]不同，此时只有重复0次有可能匹配成功，那也就看f[i][j-2]的结果。 p[j-1] == s[i]，那我们可以重复0次或者多次，只要其中有一个结果能成功。重复0次就是上面的结果。重复多次这里比较巧妙，看作是消耗掉了一个s[i]，并且依然用p[j]去做后面的匹配，也就是f[i-1][j]，只要它能匹配，那重复多次就能匹配。综合起来，f[i][j]=f[i][j-2] or f[i-1][j]，这里是比较绕的，想了很久。 p[j] == s[i] or p[j] == &amp;lsquo;.&amp;rsquo;，当前字符能匹配，所以匹配结果看f[i-1][j-1]，如果它是true，f[i][j]也是true，否则也是白瞎。
字符不匹配，那么f[i][j]=false;
但即便是理解完这一些系列的状态转换后，写代码时我依然遇到很多阻碍。最麻烦的是边界条件没理解清楚，一开始我是从下标0开始，dp中0表示第一个字符，但这也导致空串并没有被表示，而且dp过程中的数组下标越界的边界情况判断总是不对。后来看了很多解析才恍悟，首先空s或者空p的情况一定要表示出来，此时字符下标从1开始就很方便了，0表示空。
2维数组申请时，维度比s，p的长度+1，这里又引入第二个重点，要把i = 0和j = 0这两边界的元素给计算好，这样dp过程中能直接访问它们的结果。dp[0][0]=true，空串配空串。dp[i][0]这一列都是false（除了0，0处）。dp[0][j]这一行也有点绕，空串和什么样的模式能匹配呢？只有*才行了，所以一切p[j] !=*都false，等于*的还得看dp[0][j-2]是不是匹配成功（是否也为*）。
最后有个地方就是，比如pattern有a*，我们把a和*都单独判断了一次，这样不会有问题是因为我们遇到*后dp会去看j-2的结果，j-1被直接跳过了，也无法影响其他有效结果。明天继续加油，坚持刷题！
class Solution { public: bool isMatch(string s, string p) { // start from 1, 0 stands for empty case char **dp = new char*[s.length() + 1]; for (int i = 0; i &amp;lt; s.length() + 1; ++i) { dp[i] = new char[p.</description></item><item><title>理解 Inigo Quilez的 SDF 函数</title><link>http://yaotouge.github.io/posts/sdf-functions/</link><pubDate>Sun, 31 Jul 2022 11:01:47 +0000</pubDate><guid>http://yaotouge.github.io/posts/sdf-functions/</guid><description>Inigo Quilez 大佬的博客中有一篇常用 SDF 函数的合集，仔细阅读下来感觉充满了机智，所以打算写下自己的一些理解。
Box 长方体是执念比较深的一个，因为曾经面试的时候没有答出来，直接被鄙视。。。可能这也是为什么我要写这篇文章的原因之一。
float sdBox( vec3 p, vec3 b ) { vec3 q = abs(p) - b; return length(max(q,0.0)) + min(max(q.x,max(q.y,q.z)),0.0); } 机智的点在于，box是一个中心对称的几何体，对于中心在原点的box，我们可以利用对称将原本八个不同的象限缩减为一个，大大简化计算（在其他的SDF中也经常利用对称性）。
b是box的extent，也就是size的一半。第一行 abs(p) - b 把八个象限的不同情况转化为只考虑第一象限中p 与八分之一box的相对位置。我们可以想象不论p在哪个象限，其SDF的计算结果只和坐标的abs有关，与符号无关。
第二行则是把在box内和在box外的距离计算合并：
如果p在box内,q.xyz都是负，SDF的结果应该是其与box几个面最近距离取负，即 max(q.xyz)，但如果p在box外（max(q.xyz)不为负），这一项应该为0被忽略，所以应该为 min(max(q.xyz), 0)。 如果p在box外，对SDF产生贡献的只有q.xyz中大于0的分量，此处可以用2D情况想象以下。 所以计算为 length(max(q, 0))，若p在box内，这一项本身就是0，很符合我们的要求。 把两项相加就是最终结果：length(max(q, 0))+min(max(q.xyz),0)。
之前面试被问的时候，我第一反应是拿if else把八个象限全写一遍，最后还被绕晕了没写出来，这里只用2行代码就搞定，只能说智力被碾压。。。
在基本的box上加以变化，还能衍生出其他形状。
Round Box：
float sdRoundBox( vec3 p, vec3 b, float r ) { vec3 q = abs(p) - b; return length(max(q,0.0)) + min(max(q.x,max(q.y,q.z)),0.0) - r; } 唯一的变化是return的时候减r，第一反应是：这就完啦？？？细想确实如此，给box加上半径为r的圆角，如果p在box外，原本距离为d，现在距离是d-r。如果在box内，原本距离是d，现在是d + -r，都是d-r。</description></item><item><title>GPU Flow-Control Idioms 笔记</title><link>http://yaotouge.github.io/posts/gpu-branch-strategies/</link><pubDate>Mon, 09 Aug 2021 00:21:28 +0000</pubDate><guid>http://yaotouge.github.io/posts/gpu-branch-strategies/</guid><description>一直只知道shader中用分支会有额外开销，但总想搞清楚，GPU执行分支的时候发生了什么。这点不同架构，不同厂商的GPU底层细节可能都不同，他们也不会公开太多消息。我找到GPU Gems2的一篇老文章学习（貌似是好多年前的，里面还在讲GeForce 6系列显卡，有些内容可能比较过时，不过基本原理应该还是有参考价值的），做笔记记录一下。
CPU和GPU的分支模型 首先看看CPU分支的性能开销，CPU的指令流水线通常比较长（能达到10到20个时钟周期），为了让效率最大化，理想情况是不希望流水线被打断的。所以CPU的分支预测很重要。如果没有分支预测，在条件跳转（condition jump）执行完毕之前，CPU必须等待，流水线只能空着。为了改善这个问题，CPU会对分支进行预测，根据预测结果提前让指令进入流水线执行，如果预测失败，也不过是把提前执行的指令中断，重新让正确的指令进入流水线。
在GPU上，最新的GPU（那个年代还是GeForce 6xxx系列）也有类似的分支指令，但性能与CPU有些许不同。更老的GPU则没有原生的分支指令，而是用其他方式模拟分支的行为。并行架构中最常见的机制是SIMD（单指令多数据）和MIMD（多指令多数据）。SIMD架构中的所有处理单元在同一时刻执行相同的指令。MIMD中不同的处理器则可以同时执行不同的指令。GPU目前用三种方式来实现分支：MIMD分支，SIMD分支和条件码。
MIMD
MIMD是最理想的情况，每个处理单元可以像CPU那样，在没有性能损失的情况下执行不同的分支代码，GeForce 6系列的顶点处理器中支持MIMD分支。
SIMD
SIMD支持程序中出现分支以及循环，但一组中所有处理器执行完全一致的指令，所以不同处理器执行出现分歧(divergent)时性能会损失。举个例子，如果一个片元着色器在条件判断中，根据输入的随机数来决定执行哪个分支，然后输出不同的值。那么同一组处理器中，条件为False的处理器，在条件为True的处理器执行完毕之前就必须等待（SIMD中的一组处理器执行完全相同的代码，先当作全部为True执行一遍，对于False的那些处理器，会抛弃本轮的结果，然后全部当作False执行一遍）。结果就是总时间等于两个分支的总和加上条件判断指令的时间。
因此SIMD在分支条件的分布比较“连续”的时候相当有用（比如大多数warp里的处理器都走相同的分支，只有少数warp发生分歧），如果条件分布很不“连贯”（比如每个warp里面有一个处理器走不同的分值），性能会严重下降，变成两个分支的消耗之和。
条件码
在更老的GPU中采用条件码的方式来模拟分支的行为，条件判断执行后会设置条件码，if的true和false分支全都会被执行（不像SIMD中只有发生分歧才会这样），随后根据条件码的状态决定采用哪个分支的结果。所以这种架构中，分支的代价一定等于所有分支的执行时间之和加上条件判断的时间，如果每个分支的指令很少，这种方式的效率还能接受。当分支指令变得很复杂时，就需要采取其他方式了。所以这种架构中应该尽可能的避免分支。
控制流的基本策略 将分支前移 由于GPU的分支的机制相当复杂，我们需要有不同策略来应对各种情况，一种比较有效的就是将条件判断尽可能前移，在越早的阶段做判断效率就越高。
静态分支的粒度
在CPU上进行数据运算时，很多人都知道要尽可能避免在内层循环钟使用分支，否则当分支预测错误时会导致流水线的停滞，不断地重新填充指令流水线。例如在离散空间网格中计算偏微分方程（PDE）时，最直接的CPU实现就是遍历所有网格，然后判断每个网格是否为区域的边界，并使用不同的计算方法。更好的实现是用两个循环，一个遍历区域内部的网格，一个遍历边界网格，相当于将分支判断提到循环外面。
这种策略对于GPU而言更为重要，因为GPU的分支开销更大。在GPU实现中会把计算分为两个fragment program（那时还没有compute shader，GPGPU都是通过fragment shader计算的，还需要调用绘制命令），一个计算区域内部的网格，绘制内部的网格四边形，一个计算区域边界的网格，绘制边界线。
预计算
在上一个例子中，分支的结果在大部分的输入/输出值中都是不变的。有时候分支结果在一个时间段内是不变的，这种情况下，我们可以只在分支发生变化的时候进行计算，然后将结果预存起来供后面使用。
在NVIDIA SDK的 gpgpu_fluid例子中，在计算障碍物的边界处时就采用了这种技巧。当流体计算的网格与障碍物相邻时需要做额外的计算，我们必须检查相邻的网格来确定障碍物的朝向，然后根据朝向进行下一步计算。这些障碍物只有在用户进行操作时才会发生改变，因此我们可以将朝向的计算结果保存下来并复用，直到障碍物发生变化才重新计算。
Z-Cull 如果在预计算的基础上更进一步，我们可以采用另一个GPU的特性来完全跳过不必要的计算。z-cull是一种用于避免对不可见的fragment进行着色的技术（就是early-z），如果某个fragment没有通过深度测试，则在进行着色计算前取消掉这个fragment，避免不必要的计算。
在GPGPU中也可以利用这个特性，还是在流体计算的那个例子中，有的计算网格是完全“包裹”在障碍物内部的，我们并不需要对它们进行任何计算。为了跳过这些网格，当障碍物发生变化时，我们会通过一个fragment shader来检查所有的网格以及其邻居，discard那些没有被完全包裹的网格，对于被包裹的网格，则会输出0到z-buffer中。而其他的cell则用深度1画出来，这样当我们用深度0.5绘制一个全屏的quad时，障碍物内的cell就被mask掉了。
要注意的是，z-cull的分辨率通常比屏幕分辨率要粗糙（类似Hi-Z中的高等级的mipmap），所以只有一小片区域中的fragment都没有通过深度测试才会被discard掉，不同GPU的z-culling的分辨率也不同，但总的来说，只有当分支的条件（是否通过深度测试）具有一定的局部性时，z-cull才会有性能的提升。
为了展示这种局部性，我们进行一个实验，对比在通过深度测试的fragment比例相同的条件下，z值分布具有高度的空间局部性，和随机分布这两种情况下z-cull的性能，如下图。可以观察到，具有空间局部性的情况下效率是最高的，时间消耗与执行的fragment数量几乎是线性关系。而随机的情况下，即时通过深度测试的fragment比例不是太高，依然会有较多的时间消耗，好消息是当这个比例很高或者很低时（曲线的两端），天然就具备了这种空间局部性。但如果是其他情况，该方法的性能并不必条件码好多少。
最后一点要注意的是z-cull和fragment shader中的分支区别还是很大的，z-cull是完全避免fragment shader的执行，比fragment中的分支更早，节省更多资源。如果分支的条件是静态的预计算好的，z-cull是一个不错的选择。
分支指令 第一个在fragment shader中支持分支的是GeForce 6系列，随着越来越多的GPU支持分支，条件码的方式会逐渐被淘汰。但是类似前面z-cull中所展示的局部性问题那样，GPU分组并行的执行fragment shader，组内只能执行相同的代码，如果分支发生分歧，就会退化成条件码的方式，下图很好的表示这个问题：
遮挡查询 硬件遮挡查询也是设计用于防止绘制不可见fragment的一个特性。通过它可以查询渲染过程中有多少像素更新了，并且不会阻塞GPU的流水线。而通常在GPGPU计算中，计算所覆盖到的像素数量都是已知的，所以通过硬件遮挡查询，我们可以知道更新的fragment数量以及discard的数量，然后根据查询结果在CPU端做一些决策，比如作为循环的终止条件等等。
总结 虽然GPU上能支持分支指令了，但是不“紧凑”的分支计算依然还有性能损失，采用诸如预计算，将分支提到流水线前端（不管是vertex shader或者是到CPU上）的技巧，仍然是很有必要的。</description></item><item><title>GPU节能技术</title><link>http://yaotouge.github.io/posts/gpu-energy/</link><pubDate>Mon, 29 Mar 2021 23:58:21 +0000</pubDate><guid>http://yaotouge.github.io/posts/gpu-energy/</guid><description>最近在学习GPU的一些知识，看到一篇论文讲的挺清楚。但自己对芯片底层设计不太了解，只能看个大概（其实只是把论文的综述认真看了一下，哈哈，不过对目前的移动GPU有个大概的认识吧），做笔记总结一下：
移动设备以及GPU的能耗占比（图片来自于论文）： 移动GPU的渲染方式： Tegra的GPU采用立即渲染模式（Immediate-Mode Rendering) Mali采用Tile Based Rendering。 PowerVR采用也是Tile Based Rendering，但着重解决了 overdraw的问题 Adreno高通的采用混合架构，会在立即模式和tile based之间运行时切换，取名FlexRender技术。 还有其他的小众GPU，如Vivante，把GUI相关的任务交给了单独的处理器，从GPU中剥离，很好地降低GPU的功耗。 Digital Medial Professional的PICA-200 GPU，用在任天堂3DS上。 移动GPU的两大功耗杀手： Big register file A 2MB register file consumes more than 9 watts. GPU采取高并行的策略来抵消高内存延迟的影响，而高并行则导致大的register file（每个 GPU thread都需要对应的register file），导致功耗增加。移动GPU将thread的数量从几万降低到几百的数量级，性能降低了很多，为了降低功耗实属无奈。
Expensive off-chip memory access 移动GPU访问 off-chip 内存（移动GPU没有显存，或者内存当做显存用？）比访问on-chip内存的功耗要高一个数量级的功耗，甚至占用了一半以上的GPU总功耗。
目前的GPU节能技术概况： memory latency Register file optimization 没看太懂，提到了 register file cache，通过cache减少对 main register file的访问，然后通过优化GPU thread的调度，使得active thread数量减少，进一步减少register file的大小。
Prefetch 减少cache miss造成的 memory latency，与CPU的prefetch算法不同，mobile gpu也是以节能为主的。CPU的prefetch算法通常性能很好，但是取到很多不必要的数据，所以能耗表现差。
Decouple Access/Execute 把memory access和执行计算两个步骤解耦，prefetch可以提前很多，这样cache miss和计算可以overlap，不造成stall，虽然也是对prefetch的优化，但其它prefetch的地址是预测的，这种方式的prefetch地址却是计算得到的，因此更精确，造成的浪费更少。后面没看太懂。。。反正因为限制很多，实际效果没有那么好，好像并没有商业CPU采用，但好像也没说GPU上应用没有。</description></item><item><title>reversed-z</title><link>http://yaotouge.github.io/posts/reversed-z/</link><pubDate>Fri, 19 Mar 2021 18:13:45 +0000</pubDate><guid>http://yaotouge.github.io/posts/reversed-z/</guid><description>之前说到z-fighting，我只能想到两个解决方法，一个是把近平面推远，调整z值在视锥里的分布。另一种是用cascade的方式，多级视锥来把z的精度匀开，其实也是变相把近平面推远了，只不过有好几个近平面而已。
最近学习到的一个技术，reversed-z，感觉是一种更行之有效的办法。通过对投影矩阵的调整，很巧妙的把深度缓冲中的z在世界空间中的分布，和浮点数的精度分布结合起来，让深度的精度更合理。
传统的透视投影形如：
$$M_p P_w=\begin{bmatrix} s &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; s &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; \frac{f}{f-n} &amp;amp; -\frac{fn}{f-n} \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix}$$
$$d=\frac{f}{f-n}-\frac{fn}{f-n} * \frac{1}{z}$$
d为乘以投影矩阵后，再经过透视除法，写入深度缓冲的值，0对应near，1对应far。
d是一个浮点数，浮点数有一个让人比较头疼的特性，就是其所表述的值越大，精度就越低。因为底数的bits只表示小数，大数值完全是靠着指数撑起来的，而且指数可以为负，因此0附近的精度尤其高。
我们如果将float32在[0, 1]区间上的精度上可视化，如下图，蓝色的刻度表示每次给底数加上1 &amp;laquo; 20后，实际所表示的数值。（模拟每次给底数加1，但那样刻度数量太多，间距也不够明显）：
越接近0，数精度越高，蓝色刻度越密集，越接近1，精度越低，刻度间隔越大。
对应的，z-buffer值与word z的函数曲线上，将精度可视化，橙色十字表示z-buffer中的精度分布，绿色十字表示每个橙色十字对应到word z的值：
这下更明显了，函数曲线本身的特点就是，word z接近near的部分占用了[0, 1]]区间的一大半值域，再加上浮点数的精度分布在0附近更高，二者互相增益，导致z-buffer的精度在世界空间的分布更集中于near附近，极度不均匀。
实验做到这里，不得不佩服前人的智慧了，既然这两个效果互相增益，导致精度更不均匀，那如果能让二者互相抵消，是不是能够很大程度的缓解这个问题呢？于是有人想办法将z-buffer的值反过来，0对应far，1对应near，这样浮点的精度分布和透视投影的z值函数曲线的分布就能互相抵消了。这里对projection矩阵做了修改，左乘一个转换矩阵：
$$T M_p P_w=\begin{bmatrix} 1&amp;amp;0&amp;amp;0&amp;amp;0 \\ 0&amp;amp;1&amp;amp;0&amp;amp;0 \\ 0&amp;amp;0&amp;amp;-1&amp;amp;1 \\ 0&amp;amp;0&amp;amp;0&amp;amp;1 \end{bmatrix} \begin{bmatrix} s &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; s &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; \frac{f}{f-n} &amp;amp; -\frac{fn}{f-n} \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix}$$</description></item><item><title>测试mark down图床功能</title><link>http://yaotouge.github.io/posts/first/</link><pubDate>Wed, 10 Mar 2021 13:35:47 +0000</pubDate><guid>http://yaotouge.github.io/posts/first/</guid><description>终于开始在pages服务上写文章了，学了一下hexo的用法，挑了一个对眼的主题，现在就差Mark Down图片怎么搞了。 付费方案有不少，什么腾讯云、阿里云、网易云的各种对象存储，有的还有不少免费空间，但是都需要自己注册一个备案域名。我只是想在Pages上写写文章而已，于是作罢。
后来看到有两个免费方案，微博和B站的，就是不知道稳定性怎么样，要是用着用着挂了，那真是GG了，不管怎么样，先测测看。
微博的：
B站的
目前看着微博的似乎靠谱一点，需要登录自己账号，B站的都不用登录，也不知道怎么办到的。希望能多用一段时间吧，这就涉及到我要随时把自己的图片给备份好了，打算先归档，放到OneDrive里面。
写到这里，突然想到，OneDrive不是也有文件共享链接的功能吗！何不尝试一下用OneDrive做图床呢！如下：
实测了一下，确实可以，不过有两步，把文件放进OneDrive之后，先分享得到一个链接，但是这个链接并不能直接使用。于是还需要第二步，打开OneDrive的分享链接，选择顶部的嵌入，这样就可以生成一个能直接访问的图片链接，直接插入Mark Down即可。两步也不算很麻烦，但是OneDrive的网页版需要梯子才能打开，所以还是有些不便。就数据安全性，绝对是完胜上面的B站和微博了，不担心突然就跪了。
既然OneDrive可以，那百度云盘可不可以呢（邪恶笑）。尝试了一下百度云的分享链接：
百度云盘就一言难尽了，现在它现在只支持分享私密链接，也就是必须用提取码才能打开，我尝试了一下，使用提取码后，直接复制下载的链接，短时间内是可以使用的，过了一会儿就不行，看来是一个临时链接。
总结起来就是，免费的只有OneDrive能可靠使用，但得有梯子才行。微博和B站的能用，就是不知道长期稳定性如何，万一挂了，除非自己全部有备份，不然就GG。百度盘基本不能用。</description></item><item><title>About</title><link>http://yaotouge.github.io/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://yaotouge.github.io/about/</guid><description>好好生活，好好摸鱼。</description></item></channel></rss>